# -*- coding: utf-8 -*-
"""BERT-revanche.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/102rJvATPEwKrb9QFDjc0kirSw2Abw_X0
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from tqdm import tqdm

from google.colab import drive
import glob

drive.mount("/content/drive")
CORPUS_DIR = "/content/drive/My Drive/Data-philo/"
corpus_files = glob.glob(f"{CORPUS_DIR}/*/*.txt")
len(corpus_files)

corpus = {"text": [], "label": []}
for file in corpus_files:
    with open(file, "r") as f:
        for sentence in f.readlines():
            corpus["text"].append(sentence)
            corpus["label"].append(file.split("/")[-2])

corpus_df = pd.DataFrame(corpus)
corpus_df.head()

corpus_df = corpus_df.sample(frac=1, random_state=42).reset_index(drop=True)
corpus_df

label_encoder = LabelEncoder()
corpus_df["label_encoded"] = label_encoder.fit_transform(corpus_df["label"])
corpus_df

print(len(corpus_df.query("`label_encoded` == 0")))
print(len(corpus_df.query("`label_encoded` == 1")))

train_texts, val_texts, train_labels, val_labels = train_test_split(
    corpus_df["text"].values,
    corpus_df["label_encoded"].values,
    test_size=0.2,
    random_state=42,
)

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")


class ClassificationDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "label": torch.tensor(label, dtype=torch.long),
        }


# Define dataset
max_len = 128
train_dataset = ClassificationDataset(train_texts, train_labels, tokenizer, max_len)
val_dataset = ClassificationDataset(val_texts, val_labels, tokenizer, max_len)

# Create DataLoaders
batch_size = 16
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Model and Device Setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased", num_labels=len(label_encoder.classes_)
)
model.to(device)

# Optimizer
optimizer = AdamW(model.parameters(), lr=5e-5)

# Training Loop
epochs = 3
for epoch in range(epochs):
    model.train()
    total_loss = 0
    loop = tqdm(train_loader, leave=True)

    for batch in loop:
        optimizer.zero_grad()
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["label"].to(device)

        outputs = model(
            input_ids=input_ids, attention_mask=attention_mask, labels=labels
        )
        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()

        loop.set_description(f"Epoch {epoch}")
        loop.set_postfix(loss=loss.item())

    print(f"Epoch {epoch} Loss: {total_loss / len(train_loader)}")

# Evaluation
model.eval()
correct = 0
total = 0

with torch.no_grad():
    for batch in val_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["label"].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        predictions = torch.argmax(outputs.logits, dim=1)

        correct += (predictions == labels).sum().item()
        total += labels.size(0)

accuracy = correct / total
print(f"Validation Accuracy: {accuracy:.4f}")

model.save_pretrained("/content/drive/My Drive/BERT-model/bert_phrases_classifier")
tokenizer.save_pretrained("/content/drive/My Drive/BERT-model/bert_phrases_classifier")
torch.save(label_encoder, "/content/drive/My Drive/BERT-model/label_encoder.pth")

"""# INFERENCE"""

# Load Model and Tokenizer
model = BertForSequenceClassification.from_pretrained(
    "/content/drive/My Drive/BERT-model/bert_phrases_classifier"
).to(device)
tokenizer = BertTokenizer.from_pretrained(
    "/content/drive/My Drive/BERT-model/bert_phrases_classifier"
)
label_encoder = torch.load(
    "/content/drive/My Drive/BERT-model/label_encoder.pth", map_location=device
)

# Sample Input
sample_text = "Si l'on suit la thèse des sceptiques, l'homme n'aura jamais la certitude de savoir s'il a atteint la vérité, il se trouve donc dans un doute permanent. Pour sortir de ce doute mieux vaut suspendre le jugement . En conséquence , la thèse des sceptiques aboutit à une impasse puisque l'homme ne pouvant pas connaître, il ne pourra pas maîtriser tout se qui l'entoure. Les nouveaux sceptiques ont transmis 5 modes de la suspension du jugement. Le premier mode, est la discordance, et c'est à travers lui que les sceptiques préconisent, réclament l'arrêt du jugement, donc l'arrêt de la volonté de connaître. Les sceptiques considèrent qu'il faut en arriver 'à chacun sa vérité '. Le deuxième mode est la régression a à l'infinie. c-a-d l'incertitude permanent et la suspension du jugement, puisqu'il est impossible de connaitre une vérité car il nous manque des éléments pour bien connaitre. "
inputs = tokenizer(
    sample_text,
    return_tensors="pt",
    padding="max_length",
    truncation=True,
    max_length=128,
).to(device)

# Prediction
outputs = model(**inputs)
predicted_class = torch.argmax(outputs.logits, dim=1).item()
print(f"Predicted Section: {label_encoder.inverse_transform([predicted_class])[0]}")
